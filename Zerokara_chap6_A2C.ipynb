{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zerokara-chap6-A2C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvppi1H+J1IVnixZvrQZwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argonism/TsukurinagaraRL/blob/master/Zerokara_chap6_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-QKrC0DaLj"
      },
      "source": [
        "あれ、$\\pi$と$Q$ってどう違うんだっけ？\n",
        "行動確率と行動価値だから、根本的に違うかな...?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwB4talX6w7"
      },
      "source": [
        "# A2C\n",
        "- Advantage学習\n",
        "  - 今まではQ学習に1step先までの状態をみてきたが、これを2step先以上までみる\n",
        "  - $Q(s_t, a_t) \\rightarrow R(t+1) + \\gamma \\bullet R(t+2) + (\\gamma^2) \\bullet max_a[Q(s_{t+2}, a)]$\n",
        "- 分散学習(Asynchronous\n",
        "  - 非同期に複数のエージェントを学習させる。\n",
        "  - A2Cでは、一つのニューラルネットワークに対して、複数のエージェントにより学習を行い、分散学習する。\n",
        "  - 学習が効率的になる\n",
        "  - 自然とランダムなtransitionになるから、Experience Replayの必要がない\n",
        "- Actor-Critic\n",
        "  - これから詳しく。たしか、価値反復法と方策反復法を合わせたようなものみたいな説明だった気がする。\n",
        "\n",
        "## Advantage学習\n",
        "未来のstep数を増やせば増やすほど良いというわけではない\n",
        "- 未来のstep数が増える = 未確定の行動を決定する回数が増える = 間違った学習をすることも増える\n",
        "\n",
        "## 分散学習\n",
        "\n",
        "## Actor-Critic\n",
        "Actor-Criticのニューラルネットワークは、状態を受け取り、ActorとCriticを返す。\n",
        "\n",
        "Actorは行動の数だけ出力され、Criticは状態価値をあらわす一つの値のみを返す。\n",
        "\n",
        "ほうほう...\n",
        "\n",
        "つまり、Actor-CriticのNNからは{行動の種類 + 1}個の出力がある。（Cartpoleなら3)\n",
        "(+ 1は状態価値。critic分)\n",
        "\n",
        "Actorの出力は、方策反復法と同じで状態$s_t$の入力に対してその行動がどれだけ良いものかを出力する。(ということは、一つの値が出力される？）\n",
        "この出力をsoftmax関数を通せば、その行動の採用確率として利用できる。\n",
        "\n",
        "critcは状態価値。その状態になった時に、その先で得られるであろう状態価値の割引報酬和。\n",
        "\n",
        "Actor側で最大化したいのは状態$s_t$において、結合パラメータ$\\theta$を使用して行動し続けた時の割引報酬和。この割引報酬和は方策勾配法を使って\n",
        "$$J(\\theta, s_t) = E[log\\pi_\\theta(a|s)(Q^\\pi(s,a)-V_s^\\pi)]$$\n",
        "と表せる。（らしい）\n",
        "\n",
        "- $\\theta$ : それぞれの状態での行動方針（行動パターンの種類とも言える）\n",
        "- $\\pi$ : それぞれの状態での行動確率\n",
        "- $E[]$: 期待値を計算する(実装時はミニバッチの平均として表される）\n",
        "- $log\\pi_\\theta(a|s)$: 状態sのときに行動aを採用する確率のlog(行動確率のログ)\n",
        "- $Q^\\pi(s,a)$: 状態sで行動aを採用した場合の行動価値（定数として扱う）。Advantage学習で計算する\n",
        "- $V_s^\\pi$: criticの出力。状態価値\n",
        "\n",
        "### 方策のエントロピー項\n",
        "A3C, A2CではActorの学習ではさらに「方策のエントロピー項」を追加し、これは以下のように表される。\n",
        "$$Actor_{entropy} = \\sum^a[\\pi_\\theta(a|s)log\\pi_\\theta(a|s)]$$\n",
        "Σは行動の種類についての総和を計算している。\n",
        "方策が学習初期段階で、行動がランダムに決まるとき、最大となり、\n",
        "方策で行動が一方に決まっているとき、最小になる。\n",
        "Q. 目的を知った上で数式を見ても、ピンとこない。ランダムなときに最大になるのか...?\n",
        "\n",
        "学習の初期段階で勾配の局所解に落ちるのを避ける狙い。\n",
        "\n",
        "### Criticの学習\n",
        "$$loss_{Critic} = (Q^\\pi(s, a) - V_s^\\pi)^2$$\n",
        "\n",
        "この関数の最小化を目指して学習する。\n",
        "\n",
        "状態価値をその状態での行動価値の総和かな？に近づけたいっぽくて、\n",
        "確かにその状態で取れる行動の価値の総和がその状態の価値になるっているのは納得できるけど、\n",
        "行動価値を正しく設定できていることが前提となるので、可能か？という気持ち\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3dNW3vjCT6"
      },
      "source": [
        "# 使用するパッケージのインストール\n",
        "# gym==0.17.2 pyvirtualdisplay==1.3.2\n",
        "# xvfb=2:1.19.6-1ubuntu4.4 python-opengl=3.1.0+dfsg-1 ffmpeg=7:3.4.8-0ubuntu0.2\n",
        "# JSAnimation==0.1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install JSAnimation > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYHqeXSeCMp"
      },
      "source": [
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " %matplotlib inline\n",
        " import gym\n",
        "from gym.wrappers import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4DcxKAiwzI"
      },
      "source": [
        "# 動画の描画関数の宣言\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(640, 400))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def reset_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  for mp4 in mp4list:\n",
        "    os.remove(mp4)\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True, video_callable=(lambda ep: ep % 10 == 0))\n",
        "  reset_video()\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjaD9q5hy0nr"
      },
      "source": [
        "ENV = 'CartPole-v0'\n",
        "GAMMA = 0.99\n",
        "MAX_STEPS = 200\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "NUM_PROCESSES = 16\n",
        "NUM_ADVANCED_STEP = 5\n",
        "value_loss_coef = 0.5\n",
        "entropy_coef = 0.01\n",
        "max_grad_norm = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k97Fox4O6rgU"
      },
      "source": [
        "# Advantage学習のためのクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRiPKFzm5d7J"
      },
      "source": [
        "class RolloutStorage(object):\n",
        "  def __init__(self, num_steps, num_processes, obs_shape):\n",
        "    self.observations = torch.zeros(num_steps + 1, num_processes, 4)\n",
        "    # 各試行の終わりを示すための変数。次のステップが存在すれば1、なければ0\n",
        "    self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
        "    self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
        "    self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
        "\n",
        "    self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
        "    self.index = 0\n",
        "  \n",
        "  def insert(self, current_obs, action, reward, mask):\n",
        "    self.observations[self.index + 1].copy_(current_obs)\n",
        "    self.masks[self.index + 1].copy_(mask)\n",
        "    self.rewards[self.index].copy_(reward)\n",
        "    self.actions[self.index].copy_(action)\n",
        "\n",
        "    self.index = (self.index + 1) % NUM_ADVANCED_STEP\n",
        "  \n",
        "  def after_update(self):\n",
        "    ''' advantage学習分のstep数を超えたら、改めて最初から入れ直す '''\n",
        "    self.observations[0].copy_(self.observations[-1])\n",
        "    self.masks[0].copy_(self.masks[-1])\n",
        "  \n",
        "  def compute_returns(self, next_value):\n",
        "    ''' 割引報酬和を計算 '''\n",
        "    self.returns[-1] = next_value\n",
        "    # 5step目から計算していく\n",
        "    for ad_step in reversed(range(self.rewards.size(0))):\n",
        "      self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
        "        GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KNu0CF7BHn4"
      },
      "source": [
        "# ネットワーク\n",
        "## evaluate_actions\n",
        "\n",
        "> Actorの出力は、方策反復法と同じで状態 𝑠𝑡 の入力に対してその行動がどれだけ良いものかを出力する。(ということは、一つの値が出力される？） この出力をsoftmax関数を通せば、その行動の採用確率として利用できる。\n",
        "\n",
        "の通り、actor_outputをsoftmaxに通したものが行動の確率変数となる。\n",
        "\n",
        "その上で、log_softmax(x) -> log(softmax(x)) だから、\n",
        "\n",
        "log_probs: $\\log\\pi_\\theta(a|s)$\n",
        "\n",
        "action_log_probs: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SRYQ7Ip_lNc"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self, n_in, n_mid, n_out):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_in, n_mid)\n",
        "    self.fc2 = nn.Linear(n_mid, n_mid)\n",
        "    self.actor = nn.Linear(n_mid, n_out)\n",
        "    self.critic = nn.Linear(n_mid, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h1 = F.relu(self.fc1(x))\n",
        "    h2 = F.relu(self.fc2(h1))\n",
        "    critic_output = self.critic(h2)\n",
        "    actor_output = self.actor(h2)\n",
        "\n",
        "    return critic_output, actor_output\n",
        "  \n",
        "  def act(self, x):\n",
        "    # なんじゃこれ...pytorch...\n",
        "    value, actor_output = self(x)\n",
        "    action_probs = F.softmax(actor_output, dim=1)\n",
        "    # 多分確率変数(action_probs)の一番大きいやつを1つ取り出してくれる(0/1)\n",
        "    action = action_probs.multinomial(num_samples=1)\n",
        "    return action\n",
        "  \n",
        "  def get_value(self, x):\n",
        "    value, actor_output = self(x)\n",
        "    return value\n",
        "  \n",
        "  def evaluate_actions(self, x, actions):\n",
        "    ''' ネットワーク更新時に使用 '''\n",
        "    ''' epsilon-greedy法は使わず、確率的に行動を決める。 '''\n",
        "    value, actor_output = self(x)\n",
        "    log_probs = F.log_softmax(actor_output, dim=1)\n",
        "    action_log_probs = log_probs.gather(1, actions)\n",
        "\n",
        "    probs = F.softmax(actor_output, dim=1)\n",
        "    entropy = - (log_probs * probs).sum(-1).mean()\n",
        "\n",
        "    return value, action_log_probs, entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geGnmtlNax3k"
      },
      "source": [
        "\n",
        "advantages: $Q^\\pi(s,a)-V_s^\\pi$\n",
        "\n",
        "critic_loss: $(Q^\\pi(s, a) - V_s^\\pi)^2$\n",
        "\n",
        "action_gain: $J(\\theta, s_t) = E[log\\pi_\\theta(a|s)(Q^\\pi(s,a)-V_s^\\pi)]$\n",
        "\n",
        "\n",
        "$$Actor_{entropy} = \\sum^a[\\pi_\\theta(a|s)log\\pi_\\theta(a|s)]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gwBq9lI-syb"
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "class Brain(object):\n",
        "  def __init__(self, actor_critic):\n",
        "    self.actor_critic = actor_critic\n",
        "    self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\n",
        "\n",
        "  def update(self, rollouts):\n",
        "    obs_shape = rollouts.observations.size()[2:]\n",
        "    num_steps = NUM_ADVANCED_STEP\n",
        "    num_processes = NUM_PROCESSES\n",
        "\n",
        "    values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
        "        rollouts.observations[:-1].view(-1, 4),\n",
        "        rollouts.actions.view(-1, 1)\n",
        "    )\n",
        "\n",
        "    # rollouts.observationやactions.view(-1,1)は(process * advance_step)で 80x4, 80x1になる。\n",
        "\n",
        "    values = values.view(num_steps, num_processes, 1)\n",
        "    action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
        "\n",
        "    # advantages: 行動価値 - 状態価値\n",
        "    advantages = rollouts.returns[:-1] - values\n",
        "    # criticのloss\n",
        "    value_loss = advantages.pow(2).mean()\n",
        "    # actorのgainを計算するらしいが、ところでgainって何？聞いたことないけど\n",
        "    # J(theta, s_t)を計算してると思われる。action_log_probsがlog\\pi(s|a)だし。meanで期待値か。\n",
        "    action_gain = (action_log_probs * advantages.detach()).mean()\n",
        "    # 誤差関数の総和。これなんだろ。criticとactionは独立して学習させるわけじゃないのか。\n",
        "    # 説明に出てきてないと思われる\n",
        "    total_loss = (value_loss * value_loss_coef - action_gain - entropy * entropy_coef)\n",
        "\n",
        "    self.actor_critic.train()\n",
        "    self.optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
        "    # 一気に結合パラメータが変化しすぎないように、勾配の大きさは最大0.5まで\n",
        "\n",
        "    self.optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KIsLGuOgvDl"
      },
      "source": [
        "import copy\n",
        "\n",
        "class Environment:\n",
        "  def run(self):\n",
        "    ''' メインの実行 '''\n",
        "    envs = [wrap_env(gym.make(ENV)) for i in range(NUM_PROCESSES)]\n",
        "\n",
        "    n_in = envs[0].observation_space.shape[0]\n",
        "    n_out = envs[0].action_space.n\n",
        "    n_mid = 32\n",
        "    actor_critic = Net(n_in, n_mid, n_out)\n",
        "\n",
        "    global_brain = Brain(actor_critic)\n",
        "\n",
        "    obs_shape = n_in\n",
        "    current_obs = torch.zeros(NUM_PROCESSES, obs_shape)\n",
        "    rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)\n",
        "    episode_rewards = torch.zeros([NUM_PROCESSES, 1])\n",
        "    final_rewards = torch.zeros([NUM_PROCESSES, 1])\n",
        "    obs_np = np.zeros([NUM_PROCESSES, obs_shape])\n",
        "    reward_np = np.zeros([NUM_PROCESSES, 1])\n",
        "    done_np = np.zeros([NUM_PROCESSES, 1])\n",
        "    each_step = np.zeros(NUM_PROCESSES)\n",
        "    episode = 0\n",
        "\n",
        "    obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
        "    obs = np.array(obs)\n",
        "    obs = torch.from_numpy(obs).float()\n",
        "    current_obs = obs\n",
        "\n",
        "    rollouts.observations[0].copy_(current_obs)\n",
        "\n",
        "    for j in range(NUM_EPISODES * NUM_PROCESSES):\n",
        "      # Advanced学習のためのループ\n",
        "      for step in range(NUM_ADVANCED_STEP):\n",
        "        with torch.no_grad():\n",
        "          action = actor_critic.act(rollouts.observations[step])\n",
        "        actions = action.squeeze(1).numpy()\n",
        "\n",
        "        # 1step踏む\n",
        "        for i in range(NUM_PROCESSES):\n",
        "          obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n",
        "          if done_np[i]:\n",
        "            if i == 0:\n",
        "              print(f'{episode} Episode: Finished after {each_step[i] + 1} time steps')\n",
        "              episode += 1\n",
        "            \n",
        "            # 終了時の報酬の設定\n",
        "            if each_step[i] < 195:\n",
        "              reward_np[i] = -1.0\n",
        "            else:\n",
        "              reward_np[i] = 1.0\n",
        "            \n",
        "            each_step[i] = 0\n",
        "            obs_np[i] = envs[i].reset()\n",
        "\n",
        "          else:\n",
        "            reward_np[i] = 0.0\n",
        "            each_step[i] += 1\n",
        "        # 1step終了\n",
        "\n",
        "        reward = torch.from_numpy(reward_np).float()\n",
        "        episode_rewards += reward\n",
        "\n",
        "        # 最後のstepだったら0.0。それ以外なら1.0\n",
        "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done_np])\n",
        "\n",
        "        final_rewards *= masks\n",
        "\n",
        "        final_rewards += (1 - masks) * episode_rewards\n",
        "\n",
        "        episode_rewards *= masks\n",
        "\n",
        "        current_obs *= masks\n",
        "\n",
        "        obs = torch.from_numpy(obs_np).float()\n",
        "        current_obs = obs\n",
        "\n",
        "        rollouts.insert(current_obs, action.data, reward, masks)\n",
        "    # Advanced学習のためのループ終了\n",
        "      with torch.no_grad():\n",
        "        next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n",
        "      \n",
        "      rollouts.compute_returns(next_value)\n",
        "\n",
        "      global_brain.update(rollouts)\n",
        "      rollouts.after_update()\n",
        "\n",
        "      # \n",
        "      if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
        "        print('連続成功')\n",
        "        show_video()\n",
        "        break\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "vS0vFNqE3wqO",
        "outputId": "380e5267-fcab-4c1b-a25c-4113a5d68cac"
      },
      "source": [
        "cartpole_env = Environment()\n",
        "cartpole_env.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Episode: Finished after 29.0 time steps\n",
            "1 Episode: Finished after 16.0 time steps\n",
            "2 Episode: Finished after 15.0 time steps\n",
            "3 Episode: Finished after 19.0 time steps\n",
            "4 Episode: Finished after 17.0 time steps\n",
            "5 Episode: Finished after 22.0 time steps\n",
            "6 Episode: Finished after 11.0 time steps\n",
            "7 Episode: Finished after 61.0 time steps\n",
            "8 Episode: Finished after 20.0 time steps\n",
            "9 Episode: Finished after 34.0 time steps\n",
            "10 Episode: Finished after 25.0 time steps\n",
            "11 Episode: Finished after 18.0 time steps\n",
            "12 Episode: Finished after 94.0 time steps\n",
            "13 Episode: Finished after 47.0 time steps\n",
            "14 Episode: Finished after 117.0 time steps\n",
            "15 Episode: Finished after 86.0 time steps\n",
            "16 Episode: Finished after 21.0 time steps\n",
            "17 Episode: Finished after 21.0 time steps\n",
            "18 Episode: Finished after 189.0 time steps\n",
            "19 Episode: Finished after 139.0 time steps\n",
            "20 Episode: Finished after 128.0 time steps\n",
            "21 Episode: Finished after 131.0 time steps\n",
            "22 Episode: Finished after 86.0 time steps\n",
            "23 Episode: Finished after 162.0 time steps\n",
            "24 Episode: Finished after 93.0 time steps\n",
            "25 Episode: Finished after 200.0 time steps\n",
            "26 Episode: Finished after 69.0 time steps\n",
            "27 Episode: Finished after 200.0 time steps\n",
            "連続成功\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb7twQ_L-td6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}