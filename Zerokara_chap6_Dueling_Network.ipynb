{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zerokara-chap6-Dueling_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTQHgUXHQyuIQBoPDxnvRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argonism/TsukurinagaraRL/blob/master/Zerokara_chap6_Dueling_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aACPKEb6jmFP"
      },
      "source": [
        "# Dueling Network\n",
        "状態価値であるV(s)を使ったもの。\n",
        "行動によらず、次の状態が決まってしまうような状態がある。\n",
        "つまり、Q関数が持つ情報は状態Sだけで決まる部分と、行動次第で決まる部分がある。\n",
        "\n",
        "これを分離しようとしたのがDueling Network\n",
        "\n",
        "V(s)とA(s, a)に分けて学習し、最後に二つを足してQ(s,a)を求める。\n",
        "\n",
        "Q. DQNに比べて V(s)につながるネットワークの結合パラメータが行動aによらず毎step学習できるらしいが、まだよくわからん。行動の選択肢が増えるほど、大きな利点になるらしい。\n",
        "- 行動によらずに、どうやってV(s)を学習するんや。\n",
        "- ニューラルネットの中で、途中で分岐するのか。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN3dNW3vjCT6"
      },
      "source": [
        "# 使用するパッケージのインストール\n",
        "# gym==0.17.2 pyvirtualdisplay==1.3.2\n",
        "# xvfb=2:1.19.6-1ubuntu4.4 python-opengl=3.1.0+dfsg-1 ffmpeg=7:3.4.8-0ubuntu0.2\n",
        "# JSAnimation==0.1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install JSAnimation > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYHqeXSeCMp"
      },
      "source": [
        " import numpy as np\n",
        " import matplotlib.pyplot as plt\n",
        " %matplotlib inline\n",
        " import gym\n",
        "from gym.wrappers import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4DcxKAiwzI"
      },
      "source": [
        "# 動画の描画関数の宣言\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(640, 400))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "def reset_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  for mp4 in mp4list:\n",
        "    os.remove(mp4)\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True, video_callable=(lambda ep: ep % 10 == 0))\n",
        "  reset_video()\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WW-ufqkk8nL"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, n_in, n_mid, n_out):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_in, n_mid)\n",
        "    self.fc2 = nn.Linear(n_mid, n_out)\n",
        "    self.fc3_adv = nn.Linear(n_mid, n_out)\n",
        "    self.fc3_v = nn.Linear(n_mid, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h1 = F.relu(self.fc1(x))\n",
        "    h2 = F.relu(self.fc2(h1))\n",
        "    adv = self.fc3_adv(h2)\n",
        "    val = self.fc3_v(h2).expand(-1, adv.size(1))\n",
        "    # val はadvと足し算するために、サイズを[minibatchx1]から[minibatchx2(n_out)]にexpandする。\n",
        "\n",
        "    # val + adv - mean(adv)\n",
        "    output = val + adv - adv.mean(1, keepdim=True).expand(-1, adv.size(1))\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRKHopu-nVTG"
      },
      "source": [
        "## Q\n",
        "advが行動を、valが状態価値を意味するわけじゃないというか、\n",
        "特に状態価値をあらわすようにvalがあるわけじゃなくないか？\n",
        "\n",
        "つまり、なんとなく状態価値っぽくネットワークを分岐させてるけど、それが状態価値をあらわすとは限らなくない？\n",
        "Advantage関数も同様に。\n",
        "\n",
        "結局Qが出てくるだけだから...うーん...\n",
        "\n",
        "Qiitaみてたらやっぱりそうっぽいようなことが書いてあったのでやっぱり、これらが状態価値やアドバンテージ関数を表してるとは、必ずしも言えないらしい。やったぜ。\n"
      ]
    }
  ]
}